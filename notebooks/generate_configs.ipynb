{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_LANG_CORPORA = ['BAS19_ES', 'FOR19_PT', 'HAS21_HI', 'OUS19_AR', 'SAN20_IT']\n",
    "TARGET_CORPUS_SIZES = [0, 20, 200, 2000]\n",
    "LANG_TO_TEST_PATHS = {\n",
    "    'ES': ['processed/BAS19_ES/BAS19_ES_test_2000.jsonl', 'processed/MHC/MHC_ES_test_3745.jsonl'],\n",
    "    'PT': ['processed/FOR19_PT/FOR19_PT_test_2000.jsonl', 'processed/MHC/MHC_PT_test_3691.jsonl'],\n",
    "    'HI': ['processed/HAS21_HI/HAS21_HI_test_500.jsonl', 'processed/MHC/MHC_HI_test_3565.jsonl'],\n",
    "    'AR': ['processed/OUS19_AR/OUS19_AR_test_1000.jsonl', 'processed/MHC/MHC_AR_test_3570.jsonl'],\n",
    "    'IT': ['processed/SAN20_IT/SAN20_IT_test_2000.jsonl', 'processed/MHC/MHC_IT_test_3690.jsonl']\n",
    "}\n",
    "BASH_SCRIPT_HEADER = \"\"\"#!/bin/bash\n",
    "\n",
    "json=$(cat paths.json)\n",
    "configs_dir=$(echo \"$json\" | grep -o '\"configs_dir\": \"[^\"]*\"' | cut -d'\"' -f4)\n",
    "results_dir=$(echo \"$json\" | grep -o '\"output_dir\": \"[^\"]*\"' | cut -d'\"' -f4)\n",
    "data_dir=$(echo \"$json\" | grep -o '\"data_dir\": \"[^\"]*\"' | cut -d'\"' -f4)\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Monolingual Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_LANG_MODELS = ['pysentimiento/robertuito-base-uncased', 'neuralmind/bert-base-portuguese-cased', 'neuralspace-reverie/indic-transformers-hi-bert', 'aubmindlab/bert-base-arabertv02', 'Musixmatch/umberto-commoncrawl-cased-v1']\n",
    "PATH_OUT_TEMPLATE = 'monoling_target_lang/{target_lang_corpus}/examples_{target_corpus_size}/RUN{seed}/baseline/{fname}_eval.json'\n",
    "PATH_CHECKPOINT_TEMPLATE = 'monoling_target_lang/{target_lang_corpus}/examples_{target_corpus_size}/RUN{seed}'\n",
    "PATH_CHECKPOINT_TEMPLATE_0_EXAMPLES = 'monoling_target_lang/RUN{seed}'\n",
    "CONFIG_TEMPLATE = {\n",
    "    'path_out': None,\n",
    "    'dataset': {\n",
    "        'name': None,\n",
    "        'path': None\n",
    "    },\n",
    "    'predictor': {\n",
    "        'model': None,\n",
    "        'checkpoint': None,\n",
    "    },\n",
    "    'prediction_pipeline': {\n",
    "        'no_nli': True,\n",
    "        'catch_threshold': 0.5,\n",
    "        'catchers': {\n",
    "            'HSCatcherNoNLI': {\n",
    "                'threshold': 0.5\n",
    "            }\n",
    "        },\n",
    "        'comb_strat': 'only_HSCatcher'\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bash_script = open('../scripts/run_eval_monoling_target_lang.sh', 'w')\n",
    "bash_script.write(BASH_SCRIPT_HEADER)\n",
    "for target_lang_corpus, target_lang_model in zip(TARGET_LANG_CORPORA, TARGET_LANG_MODELS):\n",
    "    bash_script.write(f'# ------ {target_lang_corpus} - {target_lang_model} ------\\n')\n",
    "    lang = target_lang_corpus[-2:]\n",
    "    for size in TARGET_CORPUS_SIZES:\n",
    "        bash_script.write(f'# ---- {size} ----\\n')\n",
    "        for seed in range(1, 11):\n",
    "            for test_path in LANG_TO_TEST_PATHS[lang]:\n",
    "                testset_name = test_path.split('/')[-1]\n",
    "                config = dict(CONFIG_TEMPLATE)\n",
    "                config['path_out'] = PATH_OUT_TEMPLATE.format(\n",
    "                    target_lang_corpus=target_lang_corpus,\n",
    "                    target_corpus_size=size,\n",
    "                    fname='_'.join(testset_name.split('_')[:-2]),\n",
    "                    seed=seed\n",
    "                )\n",
    "                config['dataset']['name'] = '_'.join(testset_name.split('_')[:-2])\n",
    "                config['dataset']['path'] = test_path\n",
    "                config['predictor']['model'] = target_lang_model\n",
    "                if size == 0:\n",
    "                    config['predictor']['checkpoint'] = None\n",
    "                else:\n",
    "                    config['predictor']['checkpoint'] = PATH_CHECKPOINT_TEMPLATE.format(\n",
    "                        target_lang_corpus=target_lang_corpus,\n",
    "                        target_corpus_size=size,\n",
    "                        seed=seed\n",
    "                    )\n",
    "                path_out_dir = f'../configs/monoling_target_lang/{target_lang_corpus}/examples_{size}/RUN{seed}/'\n",
    "                path_config = os.path.join(path_out_dir, config['dataset']['name'] + '_eval.json')\n",
    "                if not os.path.exists(path_out_dir):\n",
    "                    Path(path_out_dir).mkdir(parents=True, exist_ok=True)\n",
    "                with open(path_config, 'w') as fout:\n",
    "                    json.dump(config, fout, indent=4, ensure_ascii=False)\n",
    "                path_config = '/'.join(path_config.split('/')[1:])\n",
    "                cmd_call = f'python3 src/evaluation.py --path_config \"${{configs_dir}}/{path_config}\" --gpu 0' # \"${1}/\n",
    "                bash_script.write(cmd_call + '\\n')\n",
    "bash_script.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Configs for X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_OUT_TEMPLATE = 'X/{target_lang_corpus}/examples_{target_corpus_size}/RUN{seed}/baseline/{fname}_eval.json'\n",
    "PATH_CHECKPOINT_TEMPLATE = 'X/{target_lang_corpus}/examples_{target_corpus_size}/RUN{seed}'\n",
    "PATH_CHECKPOINT_TEMPLATE_0_EXAMPLES = 'X/RUN{seed}'\n",
    "CONFIG_TEMPLATE = {\n",
    "    'path_out': None,\n",
    "    'dataset': {\n",
    "        'name': None,\n",
    "        'path': None\n",
    "    },\n",
    "    'predictor': {\n",
    "        'model': None,\n",
    "        'checkpoint': None,\n",
    "    },\n",
    "    'prediction_pipeline': {\n",
    "        'no_nli': True,\n",
    "        'catch_threshold': 0.5,\n",
    "        'catchers': {\n",
    "            'HSCatcherNoNLI': {\n",
    "                'threshold': 0.5\n",
    "            }\n",
    "        },\n",
    "        'comb_strat': 'only_HSCatcher'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bash_script = open('../scripts/run_eval_X.sh', 'w')\n",
    "bash_script.write(BASH_SCRIPT_HEADER)\n",
    "for target_lang_corpus in TARGET_LANG_CORPORA:\n",
    "    bash_script.write(f'# ------ {target_lang_corpus} ------\\n')\n",
    "    lang = target_lang_corpus[-2:]\n",
    "    for size in TARGET_CORPUS_SIZES:\n",
    "        bash_script.write(f'# ---- {size} ----\\n')\n",
    "        for seed in range(1, 11):\n",
    "            for test_path in LANG_TO_TEST_PATHS[lang]:\n",
    "                testset_name = test_path.split('/')[-1]\n",
    "                config = dict(CONFIG_TEMPLATE)\n",
    "                config['path_out'] = PATH_OUT_TEMPLATE.format(\n",
    "                    target_lang_corpus=target_lang_corpus,\n",
    "                    target_corpus_size=size,\n",
    "                    fname='_'.join(testset_name.split('_')[:-2]),\n",
    "                    seed=seed\n",
    "                )\n",
    "                config['dataset']['name'] = '_'.join(testset_name.split('_')[:-2])\n",
    "                config['dataset']['path'] = test_path\n",
    "                config['predictor']['model'] = 'cardiffnlp/twitter-xlm-roberta-base'\n",
    "                if size == 0:\n",
    "                    config['predictor']['checkpoint'] = None\n",
    "                else:\n",
    "                    config['predictor']['checkpoint'] = PATH_CHECKPOINT_TEMPLATE.format(\n",
    "                        target_lang_corpus=target_lang_corpus,\n",
    "                        target_corpus_size=size,\n",
    "                        seed=seed\n",
    "                    )\n",
    "                path_out_dir = f'../configs/X/{target_lang_corpus}/examples_{size}/RUN{seed}/'\n",
    "                path_config = os.path.join(path_out_dir, config['dataset']['name'] + '_eval.json')\n",
    "                if not os.path.exists(path_out_dir):\n",
    "                    Path(path_out_dir).mkdir(parents=True, exist_ok=True)\n",
    "                with open(path_config, 'w') as fout:\n",
    "                    json.dump(config, fout, indent=4, ensure_ascii=False)\n",
    "                path_config = '/'.join(path_config.split('/')[1:])\n",
    "                cmd_call = f'python3 src/evaluation.py --path_config \"${{configs_dir}}/{path_config}\" --gpu 0' # \"${1}/\n",
    "                bash_script.write(cmd_call + '\\n')\n",
    "bash_script.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Eval Configs for Non-NLI Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'cardiffnlp/twitter-xlm-roberta-base'\n",
    "ENGLISH_DATASETS = ['X_DEN', 'X_FEN', 'X_KEN']\n",
    "PATH_OUT_TEMPLATE = '{english_dataset}/{target_lang_corpus}/examples_{target_corpus_size}/RUN{seed}/baseline/{fname}_eval.json'\n",
    "PATH_CHECKPOINT_TEMPLATE = '{english_dataset}/{target_lang_corpus}/examples_{target_corpus_size}/RUN{seed}'\n",
    "PATH_CHECKPOINT_TEMPLATE_0_EXAMPLES = '{english_dataset}/RUN{seed}'\n",
    "CONFIG_TEMPLATE = {\n",
    "    'path_out': None,\n",
    "    'dataset': {\n",
    "        'name': None,\n",
    "        'path': None\n",
    "    },\n",
    "    'predictor': {\n",
    "        'model': None,\n",
    "        'checkpoint': None,\n",
    "    },\n",
    "    'prediction_pipeline': {\n",
    "        'no_nli': True,\n",
    "        'catch_threshold': 0.5,\n",
    "        'catchers': {\n",
    "            'HSCatcherNoNLI': {\n",
    "                'threshold': 0.5\n",
    "            }\n",
    "        },\n",
    "        'comb_strat': 'only_HSCatcher'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bash_script = open('../scripts/run_eval_bin.sh', 'w')\n",
    "bash_script.write(BASH_SCRIPT_HEADER)\n",
    "for eng_dataset in ENGLISH_DATASETS:\n",
    "    bash_script.write(f'# -------- {eng_dataset} --------\\n')\n",
    "    for target_lang_corpus in TARGET_LANG_CORPORA:\n",
    "        bash_script.write(f'# ------ {target_lang_corpus} ------\\n')\n",
    "        lang = target_lang_corpus[-2:]\n",
    "        for size in TARGET_CORPUS_SIZES:\n",
    "            bash_script.write(f'# ---- {size} ----\\n')\n",
    "            for seed in range(1, 11):\n",
    "                for test_path in LANG_TO_TEST_PATHS[lang]:\n",
    "                    testset_name = test_path.split('/')[-1]\n",
    "                    config = dict(CONFIG_TEMPLATE)\n",
    "                    config['path_out'] = PATH_OUT_TEMPLATE.format(\n",
    "                        english_dataset=eng_dataset,\n",
    "                        target_lang_corpus=target_lang_corpus,\n",
    "                        target_corpus_size=size,\n",
    "                        fname='_'.join(testset_name.split('_')[:-2]),\n",
    "                        seed=seed\n",
    "                    )\n",
    "                    config['dataset']['name'] = '_'.join(testset_name.split('_')[:-2])\n",
    "                    config['dataset']['path'] = test_path\n",
    "                    config['predictor']['model'] = MODEL_NAME\n",
    "                    if size == 0:\n",
    "                        config['predictor']['checkpoint'] = PATH_CHECKPOINT_TEMPLATE_0_EXAMPLES.format(\n",
    "                            english_dataset=eng_dataset,\n",
    "                            seed=seed\n",
    "                        )\n",
    "                    else:\n",
    "                        config['predictor']['checkpoint'] = PATH_CHECKPOINT_TEMPLATE.format(\n",
    "                            english_dataset=eng_dataset,\n",
    "                            target_lang_corpus=target_lang_corpus,\n",
    "                            target_corpus_size=size,\n",
    "                            seed=seed\n",
    "                        )\n",
    "                    path_out_dir = f'../configs/{eng_dataset}/{target_lang_corpus}/examples_{size}/RUN{seed}/'\n",
    "                    path_config = os.path.join(path_out_dir, config['dataset']['name'] + '_eval.json')\n",
    "                    if not os.path.exists(path_out_dir):\n",
    "                        Path(path_out_dir).mkdir(parents=True, exist_ok=True)\n",
    "                    with open(path_config, 'w') as fout:\n",
    "                        json.dump(config, fout, indent=4, ensure_ascii=False)\n",
    "                    path_config = '/'.join(path_config.split('/')[1:])\n",
    "                    cmd_call = f'python3 src/evaluation.py --path_config \"${{configs_dir}}/{path_config}\" --gpu 0' # \"${1}/\n",
    "                    bash_script.write(cmd_call + '\\n')\n",
    "bash_script.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Eval Configs for NLI-Baseline (Non-Strat) Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'morit/XLM-T-full-xnli'\n",
    "ENGLISH_DATASETS = ['X_NLI_DEN', 'X_NLI_FEN', 'X_NLI_KEN', 'X_NLI']\n",
    "PATH_OUT_TEMPLATE = '{english_dataset}/{target_lang_corpus}/examples_{target_corpus_size}/RUN{seed}/baseline/{fname}_eval.json'\n",
    "PATH_CHECKPOINT_TEMPLATE = '{english_dataset}/{target_lang_corpus}/examples_{target_corpus_size}/RUN{seed}'\n",
    "PATH_CHECKPOINT_TEMPLATE_0_EXAMPLES = '{english_dataset}/RUN{seed}'\n",
    "CONFIG_TEMPLATE = {\n",
    "    'path_out': None,\n",
    "    'path_hypotheses': 'hypotheses/en.json',\n",
    "    'dataset': {\n",
    "        'name': None,\n",
    "        'path': None\n",
    "    },\n",
    "    'predictor': {\n",
    "        'model': None,\n",
    "        'checkpoint': None,\n",
    "        'label_mapping': {\n",
    "            'entailment': 0,\n",
    "            'contradiction': 2\n",
    "        }\n",
    "    },\n",
    "    'prediction_pipeline': {\n",
    "        'catch_threshold': 0.5,\n",
    "        'catchers': {\n",
    "            'HSCatcher': {\n",
    "                'hypotheses_keys': [\n",
    "                    'hate',\n",
    "                    'this-text-is-hate-speech'\n",
    "                ],\n",
    "                'threshold': 0.5\n",
    "            }\n",
    "        },\n",
    "        'comb_strat': 'only_HSCatcher'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bash_script = open('../scripts/run_eval_nli_baseline.sh', 'w')\n",
    "bash_script.write(BASH_SCRIPT_HEADER)\n",
    "for eng_dataset in ENGLISH_DATASETS:\n",
    "    bash_script.write(f'# -------- {eng_dataset} --------\\n')\n",
    "    for target_lang_corpus in TARGET_LANG_CORPORA:\n",
    "        bash_script.write(f'# ------ {target_lang_corpus} ------\\n')\n",
    "        lang = target_lang_corpus[-2:]\n",
    "        for size in TARGET_CORPUS_SIZES:\n",
    "            # if size != 0 and eng_dataset == 'X_NLI':\n",
    "            #     continue\n",
    "            bash_script.write(f'# ---- {size} ----\\n')\n",
    "            for seed in range(1, 11):\n",
    "                for test_path in LANG_TO_TEST_PATHS[lang]:\n",
    "                    testset_name = test_path.split('/')[-1]\n",
    "                    config = dict(CONFIG_TEMPLATE)\n",
    "                    config['path_out'] = PATH_OUT_TEMPLATE.format(\n",
    "                        english_dataset=eng_dataset,\n",
    "                        target_lang_corpus=target_lang_corpus,\n",
    "                        target_corpus_size=size,\n",
    "                        fname='_'.join(testset_name.split('_')[:-2]),\n",
    "                        seed=seed\n",
    "                    )\n",
    "                    config['dataset']['name'] = '_'.join(testset_name.split('_')[:-2])\n",
    "                    config['dataset']['path'] = test_path\n",
    "                    config['predictor']['model'] = MODEL_NAME\n",
    "                    if eng_dataset == 'X_NLI' and size == 0:\n",
    "                        config['predictor']['checkpoint'] = None\n",
    "                    elif size == 0:\n",
    "                        config['predictor']['checkpoint'] = PATH_CHECKPOINT_TEMPLATE_0_EXAMPLES.format(\n",
    "                            english_dataset=eng_dataset,\n",
    "                            seed=seed\n",
    "                        )\n",
    "                    else:\n",
    "                        config['predictor']['checkpoint'] = PATH_CHECKPOINT_TEMPLATE.format(\n",
    "                            english_dataset=eng_dataset,\n",
    "                            target_lang_corpus=target_lang_corpus,\n",
    "                            target_corpus_size=size,\n",
    "                            seed=seed\n",
    "                        )\n",
    "                    path_out_dir = f'../configs/{eng_dataset}/{target_lang_corpus}/examples_{size}/RUN{seed}/baseline'\n",
    "                    path_config = os.path.join(path_out_dir, config['dataset']['name'] + '_eval.json')\n",
    "                    if not os.path.exists(path_out_dir):\n",
    "                        Path(path_out_dir).mkdir(parents=True, exist_ok=True)\n",
    "                    with open(path_config, 'w') as fout:\n",
    "                        json.dump(config, fout, indent=4, ensure_ascii=False)\n",
    "                    path_config = '/'.join(path_config.split('/')[1:])\n",
    "                    cmd_call = f'python3 src/evaluation.py --path_config \"${{configs_dir}}/{path_config}\" --gpu 0' # \"${1}/\n",
    "                    bash_script.write(cmd_call + '\\n')\n",
    "bash_script.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Eval Configs for NLI-FBT_tc_FC_FRS Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'morit/XLM-T-full-xnli'\n",
    "ENGLISH_DATASETS = ['X_NLI_DEN', 'X_NLI_FEN', 'X_NLI_KEN', 'X_NLI']\n",
    "PATH_OUT_TEMPLATE = '{english_dataset}/{target_lang_corpus}/examples_{target_corpus_size}/RUN{seed}/FBT_tc_FC_FRS/{fname}_eval.json'\n",
    "PATH_CHECKPOINT_TEMPLATE = '{english_dataset}/{target_lang_corpus}/examples_{target_corpus_size}/RUN{seed}'\n",
    "PATH_CHECKPOINT_TEMPLATE_0_EXAMPLES = '{english_dataset}/RUN{seed}'\n",
    "CONFIG_TEMPLATE = {\n",
    "    'path_out': None,\n",
    "    'path_hypotheses': 'hypotheses/en.json',\n",
    "    'dataset': {\n",
    "        'name': None,\n",
    "        'path': None\n",
    "    },\n",
    "    'predictors': {\n",
    "        'main': {\n",
    "            'model': None,\n",
    "            'checkpoint': None,\n",
    "            'label_mapping': {\n",
    "            'entailment': 0,\n",
    "            'contradiction': 2\n",
    "            }\n",
    "        },\n",
    "        'aux': {\n",
    "            'model': None,\n",
    "            'checkpoint': None,\n",
    "            'label_mapping': {\n",
    "            'entailment': 0,\n",
    "            'contradiction': 2\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'prediction_pipeline': {\n",
    "        'catch_threshold': 0.5,\n",
    "        'catchers': {\n",
    "        'HSCatcher': {\n",
    "            'hypotheses_keys': [\n",
    "            'hate',\n",
    "            'that-contains-hate-speech'\n",
    "            ],\n",
    "            'threshold': 0.5\n",
    "        }\n",
    "        },\n",
    "        'filters': {\n",
    "            'TargetFilter': {\n",
    "                'hypotheses_keys': [\n",
    "                    'target_characteristics'\n",
    "                ],\n",
    "                'threshold': 0.1\n",
    "            },\n",
    "            'RecSlurFilter': {\n",
    "                'hypotheses_keys': ['rec_slur'],\n",
    "                'thresholds': {'about_others': 0.5, 'neg_senti': 0.5}\n",
    "            },\n",
    "            'CSFilter': {\n",
    "                'hypotheses_keys': [\n",
    "                    'stance',\n",
    "                    'this-text-supports-[X]'\n",
    "                ],\n",
    "                'threshold': 0.5\n",
    "            }\n",
    "        },\n",
    "        'comb_strat': 'max_catch_min_filter'\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bash_script = open('../scripts/run_eval_nli_FBT_tc_FC_FRS.sh', 'w')\n",
    "bash_script.write(BASH_SCRIPT_HEADER)\n",
    "for eng_dataset in ENGLISH_DATASETS:\n",
    "    bash_script.write(f'# -------- {eng_dataset} --------\\n')\n",
    "    for target_lang_corpus in TARGET_LANG_CORPORA:\n",
    "        bash_script.write(f'# ------ {target_lang_corpus} ------\\n')\n",
    "        lang = target_lang_corpus[-2:]\n",
    "        for size in TARGET_CORPUS_SIZES:\n",
    "            bash_script.write(f'# ---- {size} ----\\n')\n",
    "            for seed in range(1, 11):\n",
    "                for test_path in LANG_TO_TEST_PATHS[lang]:\n",
    "                    testset_name = test_path.split('/')[-1]\n",
    "                    config = dict(CONFIG_TEMPLATE)\n",
    "                    config['path_out'] = PATH_OUT_TEMPLATE.format(\n",
    "                        english_dataset=eng_dataset,\n",
    "                        target_lang_corpus=target_lang_corpus,\n",
    "                        target_corpus_size=size,\n",
    "                        fname='_'.join(testset_name.split('_')[:-2]),\n",
    "                        seed=seed\n",
    "                    )\n",
    "                    config['dataset']['name'] = '_'.join(testset_name.split('_')[:-2])\n",
    "                    config['dataset']['path'] = test_path\n",
    "                    config['predictors']['main']['model'] = MODEL_NAME\n",
    "                    if eng_dataset == 'X_NLI' and size == 0:\n",
    "                        config['predictors']['main']['checkpoint'] = None\n",
    "                    elif size == 0:\n",
    "                        config['predictors']['main']['checkpoint'] = PATH_CHECKPOINT_TEMPLATE_0_EXAMPLES.format(\n",
    "                            english_dataset=eng_dataset,\n",
    "                            seed=seed\n",
    "                        )\n",
    "                    else:\n",
    "                        config['predictors']['main']['checkpoint'] = PATH_CHECKPOINT_TEMPLATE.format(\n",
    "                            english_dataset=eng_dataset,\n",
    "                            target_lang_corpus=target_lang_corpus,\n",
    "                            target_corpus_size=size,\n",
    "                            seed=seed\n",
    "                        )\n",
    "                    config['predictors']['aux']['model'] = MODEL_NAME\n",
    "                    config['predictors']['aux']['checkpoint'] = None\n",
    "                    \n",
    "                    path_out_dir = f'../configs/{eng_dataset}/{target_lang_corpus}/examples_{size}/RUN{seed}/FBT_tc_FC_FRS/'\n",
    "                    path_config = os.path.join(path_out_dir, config['dataset']['name'] + '_eval.json')\n",
    "                    if not os.path.exists(path_out_dir):\n",
    "                        Path(path_out_dir).mkdir(parents=True, exist_ok=True)\n",
    "                    with open(path_config, 'w') as fout:\n",
    "                        json.dump(config, fout, indent=4, ensure_ascii=False)\n",
    "                    path_config = '/'.join(path_config.split('/')[1:])\n",
    "                    cmd_call = f'python3 src/evaluation.py --path_config \"${{configs_dir}}/{path_config}\" --gpu 0' # \"${1}/\n",
    "                    bash_script.write(cmd_call + '\\n')\n",
    "bash_script.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate X_mNLI Baseline Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_OUT_TEMPLATE = '{english_dataset}/{target_lang_corpus}/examples_{target_corpus_size}/RUN{seed}/baseline/{fname}_eval.json'\n",
    "PATH_CHECKPOINT_TEMPLATE = '{english_dataset}/{target_lang_corpus}/examples_{target_corpus_size}/RUN{seed}'\n",
    "PATH_CHECKPOINT_TEMPLATE_0_EXAMPLES = '{english_dataset}/RUN{seed}'\n",
    "CONFIG_TEMPLATE = {\n",
    "    'path_out': None,\n",
    "    'path_hypotheses': None,\n",
    "    'dataset': {\n",
    "        'name': None,\n",
    "        'path': None\n",
    "    },\n",
    "    'predictor': {\n",
    "        'model': None,\n",
    "        'checkpoint': None,\n",
    "        'label_mapping': {\n",
    "            'entailment': 0,\n",
    "            'contradiction': 2\n",
    "        }\n",
    "    },\n",
    "    'prediction_pipeline': {\n",
    "        'catch_threshold': 0.5,\n",
    "        'catchers': {\n",
    "            'HSCatcher': {\n",
    "                'hypotheses_keys': [\n",
    "                    'hate',\n",
    "                    'this-text-is-hate-speech'\n",
    "                ],\n",
    "                'threshold': 0.5\n",
    "            }\n",
    "        },\n",
    "        'comb_strat': 'only_HSCatcher'\n",
    "    }\n",
    "}\n",
    "target_lang_corpora = ['BAS19_ES', 'HAS21_HI', 'OUS19_AR']\n",
    "paths_hypotheses = ['hypotheses/translations/es.json', 'hypotheses/translations/hi.json', 'hypotheses/translations/ar.json']\n",
    "model_names = ['morit/spanish_xlm_xnli', 'morit/hindi_xlm_xnli', 'morit/arabic_xlm_xnli']\n",
    "eng_dataset = 'X_mNLI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bash_script = open('../scripts/run_eval_mNLI_baseline.sh', 'w')\n",
    "bash_script.write(BASH_SCRIPT_HEADER)\n",
    "\n",
    "for target_lang_corpus, path_hypotheses, model_name in zip(target_lang_corpora, paths_hypotheses, model_names):\n",
    "    lang = target_lang_corpus[-2:]\n",
    "    for size in TARGET_CORPUS_SIZES:\n",
    "        bash_script.write(f'# ---- {size} ----\\n')\n",
    "        for seed in range(1, 11):\n",
    "            for test_path in LANG_TO_TEST_PATHS[lang]:\n",
    "                testset_name = test_path.split('/')[-1]\n",
    "                config = dict(CONFIG_TEMPLATE)\n",
    "                config['path_out'] = PATH_OUT_TEMPLATE.format(\n",
    "                    english_dataset=eng_dataset,\n",
    "                    target_lang_corpus=target_lang_corpus,\n",
    "                    target_corpus_size=size,\n",
    "                    fname='_'.join(testset_name.split('_')[:-2]),\n",
    "                    seed=seed\n",
    "                )\n",
    "                config['path_hypotheses'] = path_hypotheses\n",
    "                config['dataset']['name'] = '_'.join(testset_name.split('_')[:-2])\n",
    "                config['dataset']['path'] = test_path\n",
    "                config['predictor']['model'] = model_name\n",
    "                if size == 0:\n",
    "                    config['predictor']['checkpoint'] = None\n",
    "                else:\n",
    "                    config['predictor']['checkpoint'] = PATH_CHECKPOINT_TEMPLATE.format(\n",
    "                        english_dataset=eng_dataset,\n",
    "                        target_lang_corpus=target_lang_corpus,\n",
    "                        target_corpus_size=size,\n",
    "                        seed=seed\n",
    "                    )\n",
    "                path_out_dir = f'../configs/{eng_dataset}/{target_lang_corpus}/examples_{size}/RUN{seed}/baseline'\n",
    "                path_config = os.path.join(path_out_dir, config['dataset']['name'] + '_eval.json')\n",
    "                if not os.path.exists(path_out_dir):\n",
    "                    Path(path_out_dir).mkdir(parents=True, exist_ok=True)\n",
    "                with open(path_config, 'w') as fout:\n",
    "                    json.dump(config, fout, indent=4, ensure_ascii=False)\n",
    "                path_config = '/'.join(path_config.split('/')[1:])\n",
    "                cmd_call = f'python3 src/evaluation.py --path_config \"${{configs_dir}}/{path_config}\" --gpu 0' # \"${1}/\n",
    "                bash_script.write(cmd_call + '\\n')\n",
    "bash_script.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate X_mNLI Strategy Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_OUT_TEMPLATE = '{english_dataset}/{target_lang_corpus}/examples_{target_corpus_size}/RUN{seed}/FBT_tc_FC_FRS/{fname}_eval.json'\n",
    "PATH_CHECKPOINT_TEMPLATE = '{english_dataset}/{target_lang_corpus}/examples_{target_corpus_size}/RUN{seed}'\n",
    "PATH_CHECKPOINT_TEMPLATE_0_EXAMPLES = '{english_dataset}/RUN{seed}'\n",
    "CONFIG_TEMPLATE = {\n",
    "    'path_out': None,\n",
    "    'path_hypotheses': None,\n",
    "    'dataset': {\n",
    "        'name': None,\n",
    "        'path': None\n",
    "    },\n",
    "    'predictors': {\n",
    "        'main': {\n",
    "            'model': None,\n",
    "            'checkpoint': None,\n",
    "            'label_mapping': {\n",
    "            'entailment': 0,\n",
    "            'contradiction': 2\n",
    "            }\n",
    "        },\n",
    "        'aux': {\n",
    "            'model': None,\n",
    "            'checkpoint': None,\n",
    "            'label_mapping': {\n",
    "            'entailment': 0,\n",
    "            'contradiction': 2\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'prediction_pipeline': {\n",
    "        'catch_threshold': 0.5,\n",
    "        'catchers': {\n",
    "        'HSCatcher': {\n",
    "            'hypotheses_keys': [\n",
    "            'hate',\n",
    "            'that-contains-hate-speech'\n",
    "            ],\n",
    "            'threshold': 0.5\n",
    "        }\n",
    "        },\n",
    "        'filters': {\n",
    "            'TargetFilter': {\n",
    "                'hypotheses_keys': [\n",
    "                    'target_characteristics'\n",
    "                ],\n",
    "                'threshold': 0.1\n",
    "            },\n",
    "            'RecSlurFilter': {\n",
    "                'hypotheses_keys': ['rec_slur'],\n",
    "                'thresholds': {'about_others': 0.5, 'neg_senti': 0.5}\n",
    "            },\n",
    "            'CSFilter': {\n",
    "                'hypotheses_keys': [\n",
    "                    'stance',\n",
    "                    'this-text-supports-[X]'\n",
    "                ],\n",
    "                'threshold': 0.5\n",
    "            }\n",
    "        },\n",
    "        'comb_strat': 'max_catch_min_filter'\n",
    "    }\n",
    "}\n",
    "target_lang_corpora = ['BAS19_ES', 'HAS21_HI', 'OUS19_AR']\n",
    "paths_hypotheses = ['hypotheses/translations/es.json', 'hypotheses/translations/hi.json', 'hypotheses/translations/ar.json']\n",
    "model_names = ['morit/spanish_xlm_xnli', 'morit/hindi_xlm_xnli', 'morit/arabic_xlm_xnli']\n",
    "eng_dataset = 'X_mNLI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bash_script = open('../scripts/run_eval_mNLI_FBT_tc_FC_FRS.sh', 'w')\n",
    "bash_script.write(BASH_SCRIPT_HEADER)\n",
    "\n",
    "for target_lang_corpus, path_hypotheses, model_name in zip(target_lang_corpora, paths_hypotheses, model_names):\n",
    "    lang = target_lang_corpus[-2:]\n",
    "    for size in TARGET_CORPUS_SIZES:\n",
    "        bash_script.write(f'# ---- {size} ----\\n')\n",
    "        for seed in range(1, 11):\n",
    "            for test_path in LANG_TO_TEST_PATHS[lang]:\n",
    "                testset_name = test_path.split('/')[-1]\n",
    "                config = dict(CONFIG_TEMPLATE)\n",
    "                config['path_out'] = PATH_OUT_TEMPLATE.format(\n",
    "                    english_dataset=eng_dataset,\n",
    "                    target_lang_corpus=target_lang_corpus,\n",
    "                    target_corpus_size=size,\n",
    "                    fname='_'.join(testset_name.split('_')[:-2]),\n",
    "                    seed=seed\n",
    "                )\n",
    "                config['path_hypotheses'] = path_hypotheses\n",
    "                config['dataset']['name'] = '_'.join(testset_name.split('_')[:-2])\n",
    "                config['dataset']['path'] = test_path\n",
    "                # main\n",
    "                config['predictors']['main']['model'] = model_name\n",
    "                if size == 0:\n",
    "                    config['predictors']['main']['checkpoint'] = None\n",
    "                else:\n",
    "                    config['predictors']['main']['checkpoint'] = PATH_CHECKPOINT_TEMPLATE.format(\n",
    "                        english_dataset=eng_dataset,\n",
    "                        target_lang_corpus=target_lang_corpus,\n",
    "                        target_corpus_size=size,\n",
    "                        seed=seed\n",
    "                    )\n",
    "                # aux\n",
    "                config['predictors']['aux']['model'] = model_name\n",
    "                config['predictors']['aux']['checkpoint'] = None\n",
    "                path_out_dir = f'../configs/{eng_dataset}/{target_lang_corpus}/examples_{size}/RUN{seed}/FBT_tc_FC_FRS'\n",
    "                path_config = os.path.join(path_out_dir, config['dataset']['name'] + '_eval.json')\n",
    "                if not os.path.exists(path_out_dir):\n",
    "                    Path(path_out_dir).mkdir(parents=True, exist_ok=True)\n",
    "                with open(path_config, 'w') as fout:\n",
    "                    json.dump(config, fout, indent=4, ensure_ascii=False)\n",
    "                path_config = '/'.join(path_config.split('/')[1:])\n",
    "                cmd_call = f'python3 src/evaluation.py --path_config \"${{configs_dir}}/{path_config}\" --gpu 0' # \"${1}/\n",
    "                bash_script.write(cmd_call + '\\n')\n",
    "bash_script.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate M_mNLI Baseline Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_OUT_TEMPLATE = '{english_dataset}/{target_lang_corpus}/examples_{target_corpus_size}/RUN{seed}/baseline/{fname}_eval.json'\n",
    "PATH_CHECKPOINT_TEMPLATE = '{english_dataset}/{target_lang_corpus}/examples_{target_corpus_size}/RUN{seed}'\n",
    "PATH_CHECKPOINT_TEMPLATE_0_EXAMPLES = '{english_dataset}/{checkpoint_name}'\n",
    "CONFIG_TEMPLATE = {\n",
    "    'path_out': None,\n",
    "    'path_hypotheses': None,\n",
    "    'dataset': {\n",
    "        'name': None,\n",
    "        'path': None\n",
    "    },\n",
    "    'predictor': {\n",
    "        'model': None,\n",
    "        'checkpoint': None,\n",
    "        'label_mapping': {\n",
    "            'entailment': 0,\n",
    "            'contradiction': 2\n",
    "        }\n",
    "    },\n",
    "    'prediction_pipeline': {\n",
    "        'catch_threshold': 0.5,\n",
    "        'catchers': {\n",
    "            'HSCatcher': {\n",
    "                'hypotheses_keys': [\n",
    "                    'hate',\n",
    "                    'this-text-is-hate-speech'\n",
    "                ],\n",
    "                'threshold': 0.5\n",
    "            }\n",
    "        },\n",
    "        'comb_strat': 'only_HSCatcher'\n",
    "    }\n",
    "}\n",
    "target_lang_corpora = ['BAS19_ES', 'HAS21_HI', 'OUS19_AR']\n",
    "checkpoint_names = ['robertuito_xnli', 'hindi_bert_xnli', 'arabic_bert_xnli', ]\n",
    "paths_hypotheses = ['hypotheses/translations/es.json', 'hypotheses/translations/hi.json', 'hypotheses/translations/ar.json']\n",
    "model_names = ['pysentimiento/robertuito-base-uncased', 'neuralspace-reverie/indic-transformers-hi-bert', 'aubmindlab/bert-base-arabertv02']\n",
    "eng_dataset = 'M_mNLI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bash_script = open('../scripts/run_eval_M_mNLI_baseline.sh', 'w')\n",
    "bash_script.write(BASH_SCRIPT_HEADER)\n",
    "\n",
    "for target_lang_corpus, path_hypotheses, model_name, checkpoint_name in zip(target_lang_corpora, paths_hypotheses, model_names, checkpoint_names):\n",
    "    lang = target_lang_corpus[-2:]\n",
    "    for size in TARGET_CORPUS_SIZES:\n",
    "        bash_script.write(f'# ---- {size} ----\\n')\n",
    "        for seed in range(1, 11):\n",
    "            for test_path in LANG_TO_TEST_PATHS[lang]:\n",
    "                testset_name = test_path.split('/')[-1]\n",
    "                config = dict(CONFIG_TEMPLATE)\n",
    "                config['path_out'] = PATH_OUT_TEMPLATE.format(\n",
    "                    english_dataset=eng_dataset,\n",
    "                    target_lang_corpus=target_lang_corpus,\n",
    "                    target_corpus_size=size,\n",
    "                    fname='_'.join(testset_name.split('_')[:-2]),\n",
    "                    seed=seed\n",
    "                )\n",
    "                config['path_hypotheses'] = path_hypotheses\n",
    "                config['dataset']['name'] = '_'.join(testset_name.split('_')[:-2])\n",
    "                config['dataset']['path'] = test_path\n",
    "                config['predictor']['model'] = model_name\n",
    "                if size == 0:\n",
    "                    config['predictor']['checkpoint'] = PATH_CHECKPOINT_TEMPLATE_0_EXAMPLES.format(\n",
    "                        english_dataset=eng_dataset,\n",
    "                        checkpoint_name=checkpoint_name\n",
    "                    )\n",
    "                else:\n",
    "                    config['predictor']['checkpoint'] = PATH_CHECKPOINT_TEMPLATE.format(\n",
    "                        english_dataset=eng_dataset,\n",
    "                        target_lang_corpus=target_lang_corpus,\n",
    "                        target_corpus_size=size,\n",
    "                        seed=seed\n",
    "                    )\n",
    "                path_out_dir = f'../configs/{eng_dataset}/{target_lang_corpus}/examples_{size}/RUN{seed}/baseline'\n",
    "                path_config = os.path.join(path_out_dir, config['dataset']['name'] + '_eval.json')\n",
    "                if not os.path.exists(path_out_dir):\n",
    "                    Path(path_out_dir).mkdir(parents=True, exist_ok=True)\n",
    "                with open(path_config, 'w') as fout:\n",
    "                    json.dump(config, fout, indent=4, ensure_ascii=False)\n",
    "                path_config = '/'.join(path_config.split('/')[1:])\n",
    "                cmd_call = f'python3 src/evaluation.py --path_config \"${{configs_dir}}/{path_config}\" --gpu 0' # \"${1}/\n",
    "                bash_script.write(cmd_call + '\\n')\n",
    "bash_script.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate M_mNLI Strategy Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_OUT_TEMPLATE = '{english_dataset}/{target_lang_corpus}/examples_{target_corpus_size}/RUN{seed}/FBT_tc_FC_FRS/{fname}_eval.json'\n",
    "PATH_CHECKPOINT_TEMPLATE = '{english_dataset}/{target_lang_corpus}/examples_{target_corpus_size}/RUN{seed}'\n",
    "PATH_CHECKPOINT_TEMPLATE_0_EXAMPLES = '{english_dataset}/{checkpoint_name}'\n",
    "CONFIG_TEMPLATE = {\n",
    "    'path_out': None,\n",
    "    'path_hypotheses': None,\n",
    "    'dataset': {\n",
    "        'name': None,\n",
    "        'path': None\n",
    "    },\n",
    "    'predictors': {\n",
    "        'main': {\n",
    "            'model': None,\n",
    "            'checkpoint': None,\n",
    "            'label_mapping': {\n",
    "            'entailment': 0,\n",
    "            'contradiction': 2\n",
    "            }\n",
    "        },\n",
    "        'aux': {\n",
    "            'model': None,\n",
    "            'checkpoint': None,\n",
    "            'label_mapping': {\n",
    "            'entailment': 0,\n",
    "            'contradiction': 2\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'prediction_pipeline': {\n",
    "        'catch_threshold': 0.5,\n",
    "        'catchers': {\n",
    "        'HSCatcher': {\n",
    "            'hypotheses_keys': [\n",
    "            'hate',\n",
    "            'that-contains-hate-speech'\n",
    "            ],\n",
    "            'threshold': 0.5\n",
    "        }\n",
    "        },\n",
    "        'filters': {\n",
    "            'TargetFilter': {\n",
    "                'hypotheses_keys': [\n",
    "                    'target_characteristics'\n",
    "                ],\n",
    "                'threshold': 0.1\n",
    "            },\n",
    "            'RecSlurFilter': {\n",
    "                'hypotheses_keys': ['rec_slur'],\n",
    "                'thresholds': {'about_others': 0.5, 'neg_senti': 0.5}\n",
    "            },\n",
    "            'CSFilter': {\n",
    "                'hypotheses_keys': [\n",
    "                    'stance',\n",
    "                    'this-text-supports-[X]'\n",
    "                ],\n",
    "                'threshold': 0.5\n",
    "            }\n",
    "        },\n",
    "        'comb_strat': 'max_catch_min_filter'\n",
    "    }\n",
    "}\n",
    "target_lang_corpora = ['BAS19_ES', 'HAS21_HI', 'OUS19_AR']\n",
    "checkpoint_names = ['robertuito_xnli', 'hindi_bert_xnli', 'arabic_bert_xnli', ]\n",
    "paths_hypotheses = ['hypotheses/translations/es.json', 'hypotheses/translations/hi.json', 'hypotheses/translations/ar.json']\n",
    "model_names = ['pysentimiento/robertuito-base-uncased', 'neuralspace-reverie/indic-transformers-hi-bert', 'aubmindlab/bert-base-arabertv02']\n",
    "eng_dataset = 'M_mNLI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bash_script = open('../scripts/run_eval_M_mNLI_FBT_tc_FC_FRS.sh', 'w')\n",
    "bash_script.write(BASH_SCRIPT_HEADER)\n",
    "\n",
    "for target_lang_corpus, path_hypotheses, model_name, checkpoint_name in zip(target_lang_corpora, paths_hypotheses, model_names, checkpoint_names):\n",
    "    lang = target_lang_corpus[-2:]\n",
    "    for size in TARGET_CORPUS_SIZES:\n",
    "        bash_script.write(f'# ---- {size} ----\\n')\n",
    "        for seed in range(1, 11):\n",
    "            for test_path in LANG_TO_TEST_PATHS[lang]:\n",
    "                testset_name = test_path.split('/')[-1]\n",
    "                config = dict(CONFIG_TEMPLATE)\n",
    "                config['path_out'] = PATH_OUT_TEMPLATE.format(\n",
    "                    english_dataset=eng_dataset,\n",
    "                    target_lang_corpus=target_lang_corpus,\n",
    "                    target_corpus_size=size,\n",
    "                    fname='_'.join(testset_name.split('_')[:-2]),\n",
    "                    seed=seed\n",
    "                )\n",
    "                config['path_hypotheses'] = path_hypotheses\n",
    "                config['dataset']['name'] = '_'.join(testset_name.split('_')[:-2])\n",
    "                config['dataset']['path'] = test_path\n",
    "                # main\n",
    "                config['predictors']['main']['model'] = model_name\n",
    "                if size == 0:\n",
    "                    config['predictors']['main']['checkpoint'] = PATH_CHECKPOINT_TEMPLATE_0_EXAMPLES.format(\n",
    "                        english_dataset=eng_dataset,\n",
    "                        checkpoint_name=checkpoint_name\n",
    "                    )\n",
    "                else:\n",
    "                    config['predictors']['main']['checkpoint'] = PATH_CHECKPOINT_TEMPLATE.format(\n",
    "                        english_dataset=eng_dataset,\n",
    "                        target_lang_corpus=target_lang_corpus,\n",
    "                        target_corpus_size=size,\n",
    "                        seed=seed\n",
    "                    )\n",
    "                # aux\n",
    "                config['predictors']['aux']['model'] = model_name\n",
    "                config['predictors']['aux']['checkpoint'] = PATH_CHECKPOINT_TEMPLATE_0_EXAMPLES.format(\n",
    "                    english_dataset=eng_dataset,\n",
    "                    checkpoint_name=checkpoint_name\n",
    "                )\n",
    "                path_out_dir = f'../configs/{eng_dataset}/{target_lang_corpus}/examples_{size}/RUN{seed}/FBT_tc_FC_FRS'\n",
    "                path_config = os.path.join(path_out_dir, config['dataset']['name'] + '_eval.json')\n",
    "                if not os.path.exists(path_out_dir):\n",
    "                    Path(path_out_dir).mkdir(parents=True, exist_ok=True)\n",
    "                with open(path_config, 'w') as fout:\n",
    "                    json.dump(config, fout, indent=4, ensure_ascii=False)\n",
    "                path_config = '/'.join(path_config.split('/')[1:])\n",
    "                cmd_call = f'python3 src/evaluation.py --path_config \"${{configs_dir}}/{path_config}\" --gpu 0' # \"${1}/\n",
    "                bash_script.write(cmd_call + '\\n')\n",
    "bash_script.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

